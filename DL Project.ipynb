{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ea56d6",
   "metadata": {},
   "source": [
    "# EE-433: Deep Learning \n",
    "### Semester Project\n",
    "> Adversarial Training for Enhanced Image Recognition Security\n",
    "\n",
    "### Abstract\n",
    "> This project focuses on enhancing the security of Convolutional Neural Networks (CNNs), against adversarial attacks in image recognition tasks. We explore two adversarial attacks in our project which are Fast Gradient Sign Method (FGSM) and L0 Norm. Through experimentation we showed that mixing adversarial images with normal training data can improve systemâ€™s accuracy against adversarial attacks . Additionally, we propose leveraging the model itself to generate adversarial images for improved defense using PyTorch Framework. We demonstrate the effectiveness of these techniques in strengthening the model resilience against attacks and increase its robustness.\n",
    "\n",
    "### Team Members\n",
    "> Arooj Fatima (2020-EE-152A) <br> \n",
    "Ali Hussain (2020-EE-168A) <br>\n",
    "Muhammad Aziz Haider (2020-EE-172A) <br>\n",
    "Subhan Mansoor (2020-EE-175A)\n",
    "\n",
    "### Submitted to\n",
    "> Dr. Ahsan Tahir (Course Instructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d1383",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "\n",
    "> Imports essential libraries for data handling, and neural network construction with PyTorch Framework, including functions for  neural network modules, optimization, and data loading utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4cab058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126ba9a",
   "metadata": {},
   "source": [
    "# Importing Data & Defining Data Loader\n",
    "\n",
    "- Dataset is downloaded from https://www.kaggle.com/datasets/sameetassadullah/multi-label-image-classification-dataset <br>\n",
    "- It contains 6 classes of buildings, forests, glaciers, mountains, sea, and street  <br>\n",
    "- Test set contains 14034 images while Train set contains 3000 images <br>\n",
    "- We will also initialize data loaders for both training and test datasets using the defined transformations (resizing to 64x64 pixels, converting to tensors, and normalizing pixel values) and set up data loaders with batch size of 64 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5954d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Initialize Data Loader for Training & Test Dataset\n",
    "train_set = datasets.ImageFolder(\"dataset/train_dataset\", transform = data_transform)\n",
    "test_set = datasets.ImageFolder(\"dataset/test_dataset\", transform = data_transform)\n",
    "\n",
    "# Dataloader for the datasets\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b525acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: 14034\tTest Set: 3000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Set: {len(train_set)}\\tTest Set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36acd0d",
   "metadata": {},
   "source": [
    "# Defining CNN Architecture\n",
    "> The architecture is defined as follows <br>\n",
    "    - Four Convolutional  Layers with Kernel Size of 3x3<br>\n",
    "    - Max Pooling Layer for downsampling<br>\n",
    "    - Flattening Layer<br>\n",
    "    - Two fully connected layers with ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc30fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Max pooling layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU activation and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        \n",
    "        # Flatten the output for fully connected layers\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        # Fully connected layers with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Initialize the model\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a84e3e",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "- CrossEntropyLoss Function is used as Loss Function\n",
    "- Adam Optimizer is used for upgrading weights at a learning rate of 1x10-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d53704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTraining Loss: 1.0057662749832326\n",
      "Epoch: 2\tTraining Loss: 0.6569087514823133\n",
      "Epoch: 3\tTraining Loss: 0.5431005808440121\n",
      "Epoch: 4\tTraining Loss: 0.46400395882400597\n",
      "Epoch: 5\tTraining Loss: 0.3952080769295042\n",
      "Epoch: 6\tTraining Loss: 0.3283322825689207\n",
      "Epoch: 7\tTraining Loss: 0.26556798951192334\n",
      "Epoch: 8\tTraining Loss: 0.2016705926846374\n",
      "Epoch: 9\tTraining Loss: 0.15534703560512175\n",
      "Epoch: 10\tTraining Loss: 0.11208293611945754\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss_train = 0.0\n",
    "    \n",
    "    for imgs, lbls in train_loader:\n",
    "        outputs = model(imgs)\n",
    "        loss = loss_function(outputs, lbls)  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item()\n",
    "        \n",
    "    print('Epoch: {}\\tTraining Loss: {}'.format(epoch, loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd1ede",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "> The model gives an accuracy of 94.16% on train set while it gives an accuracy of 79.70% on test set which is pretty good to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8cbe644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 94.16%\n",
      "Accuracy test: 79.70%\n"
     ]
    }
   ],
   "source": [
    "for name, loader in [(\"train\", train_loader), (\"test\", test_loader)]:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "    print(\"Accuracy {}: {:.2f}%\".format(name, ((correct / total)*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea6819",
   "metadata": {},
   "source": [
    "## Saving the Trained CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4eca025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Trained model\n",
    "torch.save(model.state_dict(), \"trained_cnn_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af2158",
   "metadata": {},
   "source": [
    "# Adversarial Attacks\n",
    "\n",
    "> We will looking at Adversarial Attacks that can be done via the following techniques <br>\n",
    "    - Fast Gradient Sign Method (FGSM) <br> \n",
    "    - Projected Gradient Descent (PGD) <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97326b28",
   "metadata": {},
   "source": [
    "## Fast Gradient Sign Method (FGSM)\n",
    "> The Fast Gradient Sign Method (FGSM) is a technique employed in adversarial attacks on machine learning models, particularly deep neural networks. Formula has been sourced from https://www.tensorflow.org/tutorials/generative/adversarial_fgsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df2c079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, test_loader):\n",
    "    loss_function = nn.CrossEntropyLoss()    \n",
    "    \n",
    "    print(\"\\nTesting Model With Adversarial Attacks\")\n",
    "    \n",
    "    epsilons = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "    \n",
    "    for eps in epsilons: \n",
    "        adv_correct = 0        \n",
    "        total = 0              \n",
    "\n",
    "        for imgs, lbls in test_loader:\n",
    "            imgs = imgs.requires_grad_(True)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_function(outputs, lbls)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Add perturbation (noise) to images \n",
    "            grad = torch.sign(imgs.grad.data)\n",
    "            imgs_adv = torch.clamp(imgs.data + eps * grad, 0, 1)        \n",
    "            adv_outputs = model(imgs_adv)\n",
    "\n",
    "            _, adv_preds = torch.max(adv_outputs.data, 1)\n",
    "            adv_correct += (adv_preds == lbls).sum().item()\n",
    "            total += lbls.size(0)\n",
    "        print('Test Accuracy with eps: {:.2f} is {:.2f}%'.format(eps, 100 * adv_correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbe68f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Model With Adversarial Attacks\n",
      "Test Accuracy with eps: 0.00 is 55.63%\n",
      "Test Accuracy with eps: 0.05 is 28.27%\n",
      "Test Accuracy with eps: 0.10 is 16.80%\n",
      "Test Accuracy with eps: 0.15 is 12.83%\n",
      "Test Accuracy with eps: 0.20 is 10.77%\n",
      "Test Accuracy with eps: 0.25 is 9.57%\n",
      "Test Accuracy with eps: 0.30 is 8.73%\n"
     ]
    }
   ],
   "source": [
    "fgsm_attack(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d1f46",
   "metadata": {},
   "source": [
    "> The results show the modelâ€™s accuracy on a clean test set (Îµ=0.00) and on several adversarially perturbed test sets (Îµ=0.05, 0.10, 0.15, 0.20, 0.25, 0.30). As the value of epsilon increases, the accuracy of the model decreases. This suggests that the FGSM attack is successful in fooling the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59d72434",
   "metadata": {},
   "source": [
    "## Projected Gradient Descent Method (PGDM)\n",
    "> Projected Gradient Descent (PGD) is an iterative optimization algorithm used to generate adversarial examples. The goal of PGD is to find an adversarial perturbation (noise) that maximizes the loss function of the data, typically a bound on the perturbation magnitude.\n",
    "\n",
    "> Formula used to implement Project Gradient Descent (PGD) is sourced from: https://adversarial-ml-tutorial.org/adversarial_examples\n",
    "\n",
    "> In the formula equation, X(t) is the input image batch<br>Î± is the learning rate of the algorithm usually set to be some reasonably small fraction of Ïµ<br>âˆ‡ represents the gradient of the loss function with respect to the input<br>sign(â‹…) is the element-wise sign function<br>J is the loss function which in our case is built in Pytorch CrossEntropyLoss() function<br>y(true) are the labels of the input image batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "80126b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, test_loader, num_iter = 20):    \n",
    "    print(\"\\nTesting Model With Adversarial Attacks using Projected Gradient Descent (PGD)\")    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "    epsilons = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "    for eps in epsilons: \n",
    "        adv_correct = 0        \n",
    "        total = 0              \n",
    "\n",
    "        for imgs, lbls in test_loader:\n",
    "            imgs = imgs.requires_grad_(True)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_function(outputs, lbls)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Add perturbation (noise) to images using PGD\n",
    "            def gen_noise(model, imgs, lbls, epsilon, alpha, num_iter):\n",
    "                loss_function = nn.CrossEntropyLoss()\n",
    "                delta = torch.zeros_like(imgs, requires_grad=True)\n",
    "                for _ in range(num_iter):\n",
    "                    loss = loss_function(model(imgs + delta), lbls)\n",
    "                    loss.backward()\n",
    "                    delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
    "                    delta.grad.zero_()\n",
    "                return delta.detach()\n",
    "            \n",
    "            delta = gen_noise(model, imgs, lbls, epsilon=eps, alpha=1e-2, num_iter=num_iter)\n",
    "            imgs_adv = torch.clamp(imgs + delta, 0, 1)        \n",
    "            adv_outputs = model(imgs_adv)\n",
    "            \n",
    "            _, adv_preds = torch.max(adv_outputs.data, 1)\n",
    "            adv_correct += (adv_preds == lbls).sum().item()\n",
    "            total += lbls.size(0)\n",
    "        print('Test Accuracy with eps: {:.2f} is {:.2f}%'.format(eps, 100 * adv_correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6e600b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Model With Adversarial Attacks using Projected Gradient Descent (PGD)\n",
      "Test Accuracy with eps: 0.00 is 55.63%\n",
      "Test Accuracy with eps: 0.05 is 20.47%\n",
      "Test Accuracy with eps: 0.10 is 7.57%\n",
      "Test Accuracy with eps: 0.15 is 4.37%\n",
      "Test Accuracy with eps: 0.20 is 2.80%\n",
      "Test Accuracy with eps: 0.25 is 2.80%\n",
      "Test Accuracy with eps: 0.30 is 2.80%\n"
     ]
    }
   ],
   "source": [
    "model = load_pre_trained_model(\"trained_cnn_model.pth\")\n",
    "pgd_attack(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfff9e7",
   "metadata": {},
   "source": [
    "> As the value of epsilon increases, the accuracy of model decreases. This approach is more destructive than FGSM since the worst accuracy seen here is 2.80% while in FGSM, worst accuracy was 8.73% only for Ïµ=0.30."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69add60",
   "metadata": {},
   "source": [
    "# Adversarial Training\n",
    "\n",
    "> We will looking at Adversarial Training Approaches that can be done via the following techniques <br>\n",
    "    - Fast Gradient Sign Method (FGSM) <br> \n",
    "    - Projected Gradient Descent Method (PGDM) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4578940",
   "metadata": {},
   "source": [
    "## Adversarial Training -  FGSM with Fixed Epsilon Values\n",
    "\n",
    "> Here we will be using FGSM to create adversarial images with fixed epsilon values (Ïµ = 0.10, 0.20, and 0.30) and train our model on it to fine tune for adversarial attacks. Once the model is trained, we will test the model for its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec1bc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_trained_model(model_path):\n",
    "    model = CNN()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da9c3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_fixed_eps_fgsm_method(model, num_epochs, eps):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Adam Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        adv_loss_train = 0.0\n",
    "\n",
    "        for imgs, lbls in train_loader:\n",
    "            imgs.requires_grad = True  \n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            loss = loss_function(outputs, lbls)  \n",
    "            loss.backward()\n",
    "\n",
    "            # Add perturbation (noise) to images \n",
    "            grad = torch.sign(imgs.grad.data)\n",
    "            imgs_adv = torch.clamp(imgs.data + eps * grad, 0, 1)\n",
    "\n",
    "            # Compute loss and perform optimization step for adversarial examples\n",
    "            adv_outputs = model(imgs_adv)\n",
    "            adv_loss = loss_function(adv_outputs, lbls)\n",
    "            optimizer.zero_grad()\n",
    "            adv_loss.backward()\n",
    "            optimizer.step()\n",
    "            adv_loss_train += adv_loss.item()\n",
    "\n",
    "        print('Epoch: {}\\tTraining Loss (Adversarial): {:.2f}'.format(epoch, adv_loss_train / len(train_loader)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9322165d",
   "metadata": {},
   "source": [
    "### With Epsilon: 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2151063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Training with eps: 0.10\n",
      "Epoch: 1\tTraining Loss (Adversarial): 1.31\n",
      "Epoch: 2\tTraining Loss (Adversarial): 1.11\n",
      "Epoch: 3\tTraining Loss (Adversarial): 0.99\n",
      "Epoch: 4\tTraining Loss (Adversarial): 0.87\n",
      "Epoch: 5\tTraining Loss (Adversarial): 0.77\n",
      "Epoch: 6\tTraining Loss (Adversarial): 0.65\n",
      "Epoch: 7\tTraining Loss (Adversarial): 0.54\n",
      "Epoch: 8\tTraining Loss (Adversarial): 0.45\n",
      "Epoch: 9\tTraining Loss (Adversarial): 0.36\n",
      "Epoch: 10\tTraining Loss (Adversarial): 0.29\n",
      "\n",
      "Testing Model With Adversarial Attacks\n",
      "Test Accuracy with eps: 0.00 is 69.23%\n",
      "Test Accuracy with eps: 0.05 is 65.77%\n",
      "Test Accuracy with eps: 0.10 is 63.33%\n",
      "Test Accuracy with eps: 0.15 is 61.20%\n",
      "Test Accuracy with eps: 0.20 is 58.63%\n",
      "Test Accuracy with eps: 0.25 is 57.07%\n",
      "Test Accuracy with eps: 0.30 is 54.87%\n"
     ]
    }
   ],
   "source": [
    "# Load the Trained Model\n",
    "adv_train_model = load_pre_trained_model(\"trained_cnn_model.pth\")\n",
    "\n",
    "print(\"Adversarial Training with eps: 0.10\")\n",
    "train_model_with_fixed_eps_fgsm_method(model = adv_train_model, num_epochs = 10, eps = 0.10)\n",
    "\n",
    "fgsm_attack(adv_train_model, test_loader)\n",
    "\n",
    "torch.save(model.state_dict(), \"adv_trained_cnn_model_eps_010.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb0ac8",
   "metadata": {},
   "source": [
    "### With Epsilon: 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3753300d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Training with eps: 0.20\n",
      "Epoch: 1\tTraining Loss (Adversarial): 1.48\n",
      "Epoch: 2\tTraining Loss (Adversarial): 1.24\n",
      "Epoch: 3\tTraining Loss (Adversarial): 1.11\n",
      "Epoch: 4\tTraining Loss (Adversarial): 0.99\n",
      "Epoch: 5\tTraining Loss (Adversarial): 0.85\n",
      "Epoch: 6\tTraining Loss (Adversarial): 0.77\n",
      "Epoch: 7\tTraining Loss (Adversarial): 0.66\n",
      "Epoch: 8\tTraining Loss (Adversarial): 0.57\n",
      "Epoch: 9\tTraining Loss (Adversarial): 0.47\n",
      "Epoch: 10\tTraining Loss (Adversarial): 0.38\n",
      "\n",
      "Testing Model With Adversarial Attacks\n",
      "Test Accuracy with eps: 0.00 is 62.97%\n",
      "Test Accuracy with eps: 0.05 is 62.17%\n",
      "Test Accuracy with eps: 0.10 is 64.80%\n",
      "Test Accuracy with eps: 0.15 is 65.97%\n",
      "Test Accuracy with eps: 0.20 is 65.90%\n",
      "Test Accuracy with eps: 0.25 is 66.63%\n",
      "Test Accuracy with eps: 0.30 is 65.10%\n"
     ]
    }
   ],
   "source": [
    "# Load the Trained Model\n",
    "adv_train_model = load_pre_trained_model(\"trained_cnn_model.pth\")\n",
    "\n",
    "print(\"Adversarial Training with eps: 0.20\")\n",
    "train_model_with_fixed_eps_fgsm_method(model = adv_train_model, num_epochs = 10, eps = 0.20)\n",
    "\n",
    "fgsm_attack(adv_train_model, test_loader)\n",
    "\n",
    "torch.save(model.state_dict(), \"adv_trained_cnn_model_eps_020.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c36eb",
   "metadata": {},
   "source": [
    "### With Epsilon: 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3c3210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Training with eps: 0.30\n",
      "Epoch: 1\tTraining Loss (Adversarial): 1.56\n",
      "Epoch: 2\tTraining Loss (Adversarial): 1.28\n",
      "Epoch: 3\tTraining Loss (Adversarial): 1.07\n",
      "Epoch: 4\tTraining Loss (Adversarial): 0.90\n",
      "Epoch: 5\tTraining Loss (Adversarial): 0.78\n",
      "Epoch: 6\tTraining Loss (Adversarial): 0.67\n",
      "Epoch: 7\tTraining Loss (Adversarial): 0.56\n",
      "Epoch: 8\tTraining Loss (Adversarial): 0.50\n",
      "Epoch: 9\tTraining Loss (Adversarial): 0.40\n",
      "Epoch: 10\tTraining Loss (Adversarial): 0.34\n",
      "\n",
      "Testing Model With Adversarial Attacks\n",
      "Test Accuracy with eps: 0.00 is 53.20%\n",
      "Test Accuracy with eps: 0.05 is 51.50%\n",
      "Test Accuracy with eps: 0.10 is 57.13%\n",
      "Test Accuracy with eps: 0.15 is 63.23%\n",
      "Test Accuracy with eps: 0.20 is 66.50%\n",
      "Test Accuracy with eps: 0.25 is 68.57%\n",
      "Test Accuracy with eps: 0.30 is 69.43%\n"
     ]
    }
   ],
   "source": [
    "# Load the Trained Model\n",
    "adv_train_model = load_pre_trained_model(\"trained_cnn_model.pth\")\n",
    "\n",
    "print(\"Adversarial Training with eps: 0.30\")\n",
    "train_model_with_fixed_eps_fgsm_method(model = adv_train_model, num_epochs = 10, eps = 0.30)\n",
    "\n",
    "fgsm_attack(adv_train_model, test_loader)\n",
    "\n",
    "torch.save(model.state_dict(), \"adv_trained_cnn_model_eps_030.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e191a9",
   "metadata": {},
   "source": [
    "## Adversarial Training -  FGSM with Random Epsilon Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "615d1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "def train_model_with_random_eps_fgsm_method(model, num_epochs):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Adam Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        adv_loss_train = 0.0\n",
    "\n",
    "        for imgs, lbls in train_loader:\n",
    "            imgs.requires_grad = True  \n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            loss = loss_function(outputs, lbls)  \n",
    "            loss.backward()\n",
    "            \n",
    "            def get_random_eps():\n",
    "                epsilons = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "                return epsilons[randrange(0, len(epsilons))]\n",
    "            eps = get_random_eps()\n",
    "\n",
    "            # Add perturbation (noise) to images \n",
    "            grad = torch.sign(imgs.grad.data)\n",
    "            imgs_adv = torch.clamp(imgs.data + eps * grad, 0, 1)\n",
    "\n",
    "            # Compute loss and perform optimization step for adversarial examples\n",
    "            adv_outputs = model(imgs_adv)\n",
    "            adv_loss = loss_function(adv_outputs, lbls)\n",
    "            optimizer.zero_grad()\n",
    "            adv_loss.backward()\n",
    "            optimizer.step()\n",
    "            adv_loss_train += adv_loss.item()\n",
    "\n",
    "        print('Epoch: {}\\tTraining Loss (Adversarial): {:.2f}'.format(epoch, adv_loss_train / len(train_loader)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86bd438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Training with Mixed Epsilon Values at each Batch\n",
      "Epoch: 1\tTraining Loss (Adversarial): 1.44\n",
      "Epoch: 2\tTraining Loss (Adversarial): 1.21\n",
      "Epoch: 3\tTraining Loss (Adversarial): 1.14\n",
      "Epoch: 4\tTraining Loss (Adversarial): 1.04\n",
      "Epoch: 5\tTraining Loss (Adversarial): 0.90\n",
      "Epoch: 6\tTraining Loss (Adversarial): 0.82\n",
      "Epoch: 7\tTraining Loss (Adversarial): 0.72\n",
      "Epoch: 8\tTraining Loss (Adversarial): 0.65\n",
      "Epoch: 9\tTraining Loss (Adversarial): 0.56\n",
      "Epoch: 10\tTraining Loss (Adversarial): 0.48\n",
      "\n",
      "Testing Model With Adversarial Attacks\n",
      "Test Accuracy with eps: 0.00 is 68.57%\n",
      "Test Accuracy with eps: 0.05 is 64.77%\n",
      "Test Accuracy with eps: 0.10 is 62.43%\n",
      "Test Accuracy with eps: 0.15 is 61.63%\n",
      "Test Accuracy with eps: 0.20 is 60.50%\n",
      "Test Accuracy with eps: 0.25 is 60.00%\n",
      "Test Accuracy with eps: 0.30 is 60.30%\n"
     ]
    }
   ],
   "source": [
    "# Load the Trained Model\n",
    "adv_train_model = load_pre_trained_model(\"trained_cnn_model.pth\")\n",
    "\n",
    "print(\"Adversarial Training with Mixed Epsilon Values at each Batch\")\n",
    "train_model_with_random_eps_fgsm_method(model = adv_train_model, num_epochs = 10)\n",
    "\n",
    "fgsm_attack(adv_train_model, test_loader)\n",
    "\n",
    "torch.save(model.state_dict(), \"adv_trained_cnn_model_eps_rndm.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c257f4",
   "metadata": {},
   "source": [
    "## Adversarial Training -  PGDM with Random Epsilon Values\n",
    "\n",
    "> Here we will be using PGDM to create adversarial images and train our model on it to fine tune for adversarial attacks, iteratively updating the model to improve its robustness. Once the model is trained, we will test the model for its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aeb83fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "def train_model_with_random_eps_pgd_method(model, num_epochs):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Adam Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        adv_loss_train = 0.0\n",
    "\n",
    "        for imgs, lbls in train_loader:\n",
    "            imgs.requires_grad = True  \n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            loss = loss_function(outputs, lbls)  \n",
    "            loss.backward()\n",
    "            \n",
    "            def get_random_eps():\n",
    "                epsilons = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "                return epsilons[randrange(0, len(epsilons))]\n",
    "            eps = get_random_eps()\n",
    "\n",
    "            # Add perturbation (noise) to images using PGD\n",
    "            def gen_noise(model, imgs, lbls, epsilon, alpha, num_iter):\n",
    "                loss_function = nn.CrossEntropyLoss()\n",
    "                delta = torch.zeros_like(imgs, requires_grad=True)\n",
    "                \n",
    "                for _ in range(num_iter):\n",
    "                    loss = loss_function(model(imgs + delta), lbls)\n",
    "                    loss.backward()\n",
    "                    delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
    "                    delta.grad.zero_()\n",
    "                return delta.detach()\n",
    "            \n",
    "        # With learning rate to be 1e-2 and number of iterations set to 10\n",
    "            delta = gen_noise(model, imgs, lbls, epsilon = eps, alpha = 1e-2, num_iter = 10)\n",
    "            imgs_adv = torch.clamp(imgs + delta, 0, 1) \n",
    "            \n",
    "            adv_outputs = model(imgs_adv)\n",
    "            adv_loss = loss_function(adv_outputs, lbls)\n",
    "            optimizer.zero_grad()\n",
    "            adv_loss.backward()\n",
    "            optimizer.step()\n",
    "            adv_loss_train += adv_loss.item()\n",
    "\n",
    "        print('Epoch: {}\\tTraining Loss (Adversarial): {:.2f}'.format(epoch, adv_loss_train / len(train_loader)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "31577f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Training with Random Epsilon Values at each Batch using PGD Method\n",
      "Epoch: 1\tTraining Loss (Adversarial): 1.26\n",
      "Epoch: 2\tTraining Loss (Adversarial): 1.01\n",
      "Epoch: 3\tTraining Loss (Adversarial): 0.87\n",
      "Epoch: 4\tTraining Loss (Adversarial): 0.76\n",
      "Epoch: 5\tTraining Loss (Adversarial): 0.63\n",
      "\n",
      "Testing Model With Adversarial Attacks using Projected Gradient Descent (PGD)\n",
      "Test Accuracy with eps: 0.00 is 70.23%\n",
      "Test Accuracy with eps: 0.05 is 66.80%\n",
      "Test Accuracy with eps: 0.10 is 62.57%\n",
      "Test Accuracy with eps: 0.15 is 57.87%\n",
      "Test Accuracy with eps: 0.20 is 54.27%\n",
      "Test Accuracy with eps: 0.25 is 54.27%\n",
      "Test Accuracy with eps: 0.30 is 54.27%\n"
     ]
    }
   ],
   "source": [
    "# Load the Trained Model\n",
    "adv_train_model = load_pre_trained_model(\"trained_cnn_model.pth\")\n",
    "\n",
    "print(\"Adversarial Training with Random Epsilon Values at each Batch using PGD Method\")\n",
    "train_model_with_random_eps_pgd_method(model = adv_train_model, num_epochs = 5)\n",
    "\n",
    "pgd_attack(adv_train_model, test_loader)\n",
    "\n",
    "torch.save(model.state_dict(), \"adv_trained_cnn_model_eps_rndm_pgd_method.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ca28c",
   "metadata": {},
   "source": [
    "> It can be seen that the performance of the model increases drastically when it is adversarially trained on examples generated by PDGM. Without adversarially training, the model failed to predict test images for eps = 0.30, giving a worst accuracy of 2.80%. But after the adversarially training, the model successfully predicted them with confidence of 54.27%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddebb524",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "> This research-based project gives basis to further developments in the field of adversarial training of the deep neural networks by providing methods to create adversarial examples and then using them to train the model to improve its performance. We used two methods, namely <b>Fast Gradient Sign Method (FGSM)</b> and <b>Projected Gradient Descent Method (PGDM)</b> to generate adversarial examples and proved that the latter (PGDM) is much more effective than FGSM and helps the model to become more robust. With PGDM, through experimentation, we showed that the model accuracy can be increased from 2.80% to 54.27%, showing significant increase in model efficiency to detect adversarial examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
